{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "# 初始化权重参数和偏置\n",
    "weights = {} \n",
    "depth = 3  ##conv1的深度\n",
    "fc_units=84  ##全连接层\n",
    "\n",
    "## 网络结构\n",
    "## Input(1*28*28)=>convlution(3*5*5)=>relu()=>maxpooling(3*3)=>flatten()=>fullconnected(84)=>Output(10)\n",
    "weights[\"K1\"] = 1e-2 * np.random.randn(1, depth, 5, 5).astype(np.float64)\n",
    "weights[\"b1\"] = np.zeros(depth).astype(np.float64)\n",
    "weights[\"W2\"] = 1e-2 * np.random.randn(depth * 11 * 11, fc_units).astype(np.float64)\n",
    "weights[\"b2\"] = np.zeros(fc_units).astype(np.float64)\n",
    "weights[\"W3\"] = 1e-2 * np.random.randn(fc_units, 10).astype(np.float64)\n",
    "weights[\"b3\"] = np.zeros(10).astype(np.float64)\n",
    "\n",
    "# 初始化神经元和梯度\n",
    "nuerons={}\n",
    "gradients={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义前向传播\n",
    "def forward(X):\n",
    "    nuerons[\"conv1\"]=convolution_forward(X.astype(np.float64),weights[\"K1\"],weights[\"b1\"])\n",
    "    nuerons[\"conv1_relu\"]=relu_forward(nuerons[\"conv1\"])\n",
    "    \n",
    "    nuerons[\"maxp1\"]=maxpooling_forward(nuerons[\"conv1_relu\"].astype(np.float64),pooling=(3,3))\n",
    "    nuerons[\"flatten\"]=flatten_forward(nuerons[\"maxp1\"])\n",
    "    \n",
    "    nuerons[\"fc2\"]=fullyconnected_forward(nuerons[\"flatten\"],weights[\"W2\"],weights[\"b2\"])\n",
    "    nuerons[\"fc2_relu\"]=relu_forward(nuerons[\"fc2\"])\n",
    "    nuerons[\"y\"]=fullyconnected_forward(nuerons[\"fc2_relu\"],weights[\"W3\"],weights[\"b3\"])\n",
    "\n",
    "    return nuerons[\"y\"]\n",
    "\n",
    "# 定义反向传播\n",
    "def backward(X,y_true):\n",
    "    loss,dy=cross_entropy_loss(nuerons[\"y\"],y_true)\n",
    "    gradients[\"W3\"],gradients[\"b3\"],gradients[\"fc2_relu\"]=fullyconnected_backward(dy,weights[\"W3\"],nuerons[\"fc2_relu\"])\n",
    "    gradients[\"fc2\"]=relu_backward(gradients[\"fc2_relu\"],nuerons[\"fc2\"])\n",
    "    \n",
    "    gradients[\"W2\"],gradients[\"b2\"],gradients[\"flatten\"]=fullyconnected_backward(gradients[\"fc2\"],weights[\"W2\"],nuerons[\"flatten\"])\n",
    "    gradients[\"maxp1\"]=flatten_backward(gradients[\"flatten\"],nuerons[\"maxp1\"])\n",
    "       \n",
    "    gradients[\"conv1_relu\"]=maxpooling_backward(gradients[\"maxp1\"].astype(np.float64),nuerons[\"conv1_relu\"].astype(np.float64),pooling=(3,3))\n",
    "    gradients[\"conv1\"]=relu_backward(gradients[\"conv1_relu\"],nuerons[\"conv1\"])\n",
    "    gradients[\"K1\"],gradients[\"b1\"],_=convolution_backward(gradients[\"conv1\"],weights[\"K1\"],X)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义卷积层\n",
    "def convolution_forward(X_input, Kernel, b, padding=(0, 0), strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    多通道卷积前向过程\n",
    "    :param X: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数\n",
    "    :param Kernel: 卷积核,形状(C,D,k1,k2), C为输入通道数，D为输出通道数\n",
    "    :param b: 偏置,形状(D,)\n",
    "    :param padding: padding\n",
    "    :param strides: 步长\n",
    "    :return: 卷积结果\n",
    "    \"\"\"\n",
    "    padding_X = np.lib.pad(X_input, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), 'constant', constant_values=0)\n",
    "    N, _, height, width = padding_X.shape\n",
    "    C, D, k1, k2 = Kernel.shape\n",
    "\n",
    "    ## 防止出现不能整除情况，用floor函数避免\n",
    "    h_ = (height - k1) % strides[0]\n",
    "    w_ = (width - k2) % strides[1]\n",
    "\n",
    "    ##卷积之后的长度，padding为0\n",
    "    H_ = 1 + (height - k1) // strides[0]\n",
    "    W_ = 1 + (width - k2) // strides[1]\n",
    "    conv_X = np.zeros((N, D, H_, W_))\n",
    "\n",
    "    ##求和操作\n",
    "    for n in np.arange(N):\n",
    "        for d in np.arange(D):\n",
    "            for h in np.arange(height - k1 + 1):\n",
    "                for w in np.arange(width - k2 + 1):\n",
    "                    conv_X[n, d, h, w] = np.sum(padding_X[n, :, h:h + k1, w:w + k2] * Kernel[:, d]) + b[d]\n",
    "    return conv_X\n",
    "\n",
    "def convolution_backward(next_dX, Kernel, X, padding=(0, 0), strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    多通道卷积层的反向过程\n",
    "    :param next_dX: 卷积输出层的梯度,(N,D,H',W'),H',W'为卷积输出层的高度和宽度\n",
    "    :param Kernel: 当前层卷积核，(C,D,k1,k2)\n",
    "    :param X: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数\n",
    "    :param padding: padding\n",
    "    :param strides: 步长\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    C, D, k1, k2 = Kernel.shape\n",
    "\n",
    "    # 卷积核梯度\n",
    "    padding_next_dX = Zeros_padding(next_dX, strides)\n",
    "    # 增加高度和宽度0填充\n",
    "    ppadding_next_dX = np.lib.pad(padding_next_dX, ((0, 0), (0, 0), (k1 - 1, k1 - 1), (k2 - 1, k2 - 1)), 'constant', constant_values=0)\n",
    "\n",
    "    #旋转180度\n",
    "    # 卷积核高度和宽度翻转180度\n",
    "    flip_K = np.flip(Kernel, (2, 3))\n",
    "    # 交换C,D为D,C；D变为输入通道数了，C变为输出通道数了\n",
    "    switch_flip_K = np.swapaxes(flip_K, 0, 1)\n",
    "\n",
    "    ##rot(180)*W\n",
    "    dX = convolution_forward(ppadding_next_dX.astype(np.float64), switch_flip_K.astype(np.float64), np.zeros((C,), dtype=np.float64))\n",
    "\n",
    "    # 求卷积核的梯度dK\n",
    "    swap_W = np.swapaxes(X, 0, 1)  # 变为(C,N,H,W)与\n",
    "    dW = convolution_forward(swap_W.astype(np.float64), padding_next_dX.astype(np.float64), np.zeros((D,), dtype=np.float64))\n",
    "\n",
    "    # 偏置的梯度\n",
    "    db = np.sum(np.sum(np.sum(next_dX, axis=-1), axis=-1), axis=0)  # 在高度、宽度上相加；批量大小上相加\n",
    "\n",
    "    # 把padding减掉\n",
    "    dX = Zeros_remove(dX, padding)\n",
    "\n",
    "    return dW / N, db / N, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义关于激活函数Relu的前向反向传播\n",
    "\"\"\"\n",
    "def relu_forward(X):\n",
    "    \"\"\"\n",
    "    relu前向传播\n",
    "    :param X: 待激活层\n",
    "    :return: 激活后的结果\n",
    "    \"\"\"\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "\n",
    "def relu_backward(next_dX, X):\n",
    "    \"\"\"\n",
    "    relu反向传播\n",
    "    :param next_dX: 激活后的梯度\n",
    "    :param X: 激活前的值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dX = np.where(np.greater(X, 0), next_dX, 0)\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向多维数组最后两位，每个行列之间增减指定的个数的零\n",
    "#增加padding\n",
    "def Zeros_padding(dX, strides):\n",
    "    \"\"\"\n",
    "    :param dX: (N,D,H,W),H,W为卷积输出层的高度和宽度\n",
    "    :param strides: 步长\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _, _, H, W = dX.shape\n",
    "    pX = dX\n",
    "    if strides[0] > 1:\n",
    "        for h in np.arange(H - 1, 0, -1):\n",
    "            for o in np.arange(strides[0] - 1):\n",
    "                pX = np.insert(pX, h, 0, axis=2)\n",
    "    if strides[1] > 1:\n",
    "        for w in np.arange(W - 1, 0, -1):\n",
    "            for o in np.arange(strides[1] - 1):\n",
    "                pX = np.insert(pX, w, 0, axis=3)\n",
    "    return pX\n",
    "#移除padding\n",
    "def Zeros_remove(X, padding):\n",
    "    \"\"\"\n",
    "    :param X: (N,C,H,W)\n",
    "    :param paddings: (p1,p2)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if padding[0] > 0 and padding[1] > 0:\n",
    "        return X[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n",
    "    elif padding[0] > 0:\n",
    "        return X[:, :, padding[0]:-padding[0], :]\n",
    "    elif padding[1] > 0:\n",
    "        return X[:, :, :, padding[1]:-padding[1]]\n",
    "    else:\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化层，选择最大池化\n",
    "def maxpooling_forward(X, pooling, strides=(2, 2), padding=(0, 0)):\n",
    "    \"\"\"\n",
    "    最大池化前向过程\n",
    "    :param X: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数\n",
    "    :param pooling: 池化大小(k1,k2)\n",
    "    :param strides: 步长\n",
    "    :param padding: 0填充\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    # 零填充\n",
    "    padding_X = np.lib.pad(X, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), 'constant', constant_values=0)\n",
    "\n",
    "    # 输出的高度和宽度\n",
    "    H_ = (H + 2 * padding[0] - pooling[0]) // strides[0] + 1\n",
    "    W_ = (W + 2 * padding[1] - pooling[1]) // strides[1] + 1\n",
    "\n",
    "    pool_X = np.zeros((N, C, H_, W_))\n",
    "\n",
    "    for n in np.arange(N):\n",
    "        for c in np.arange(C):\n",
    "            for i in np.arange(H_):\n",
    "                for j in np.arange(W_):\n",
    "                    pool_X[n, c, i, j] = np.max(padding_X[n, c, strides[0] * i:strides[0] * i + pooling[0], strides[1] * j:strides[1] * j + pooling[1]])\n",
    "    return pool_X \n",
    "\n",
    "def maxpooling_backward(next_dX, X, pooling, strides=(2, 2), padding=(0, 0)):\n",
    "    \"\"\"\n",
    "    最大池化反向过程\n",
    "    :param next_dX：损失函数关于最大池化输出的损失\n",
    "    :param X: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数\n",
    "    :param pooling: 池化大小(k1,k2)\n",
    "    :param strides: 步长\n",
    "    :param padding: 0填充\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    _, _, H_, W_ = next_dX.shape\n",
    "    # 零填充\n",
    "    padding_X = np.lib.pad(X, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), 'constant', constant_values=0)\n",
    "\n",
    "    # 零填充后的梯度\n",
    "    padding_dX = np.zeros_like(padding_X)\n",
    "\n",
    "    for n in np.arange(N):\n",
    "        for c in np.arange(C):\n",
    "            for i in np.arange(H_):\n",
    "                for j in np.arange(W_):\n",
    "                    # 找到最大值的那个元素坐标，将梯度传给这个坐标\n",
    "                    flat_idx = np.argmax(padding_X[n, c,strides[0] * i:strides[0] * i + pooling[0], strides[1] * j:strides[1] * j + pooling[1]])\n",
    "\n",
    "                    h_idx = strides[0] * i + flat_idx // pooling[1]\n",
    "                    w_idx = strides[1] * j + flat_idx % pooling[1]\n",
    "                    padding_dX[n, c, h_idx, w_idx] += next_dX[n, c, i, j]\n",
    "    # 返回时剔除零填充\n",
    "    return Zeros_remove(padding_dX, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全连接层的前向传播\n",
    "def fullyconnected_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    :param X: 当前层的输出,形状 (N,ln)\n",
    "    :param W: 当前层的权重\n",
    "    :param b: 当前层的偏置\n",
    "    :return: 下一层的输出\n",
    "    \"\"\"\n",
    "    return np.dot(X, W) + b\n",
    "\n",
    "#全连接层的反向传播\n",
    "def fullyconnected_backward(next_dX, W, X):\n",
    "    \"\"\"\n",
    "    :param next_dX: 下一层的梯度\n",
    "    :param W: 当前层的权重\n",
    "    :param X: 当前层的输出\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    delta = np.dot(next_dX, W.T)  # 当前层的梯度\n",
    "    dw = np.dot(X.T, next_dX)  # 当前层权重的梯度\n",
    "    db = np.sum(next_dX, axis=0)  # 当前层偏置的梯度, N个样本的梯度求和\n",
    "    return dw / N, db / N, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正确率计算\n",
    "def get_accuracy(X,y_true):\n",
    "    y_predict=forward(X)\n",
    "    return np.mean(np.equal(np.argmax(y_predict,axis=-1), np.argmax(y_true,axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将多维数组展平，前向传播\n",
    "def flatten_forward(X):\n",
    "    \"\"\"\n",
    "    :param X: 多维数组,形状(N,d1,d2,..)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    return np.reshape(X, (N, -1))\n",
    "\n",
    "#反向传播展平\n",
    "def flatten_backward(next_dX, X):\n",
    "    \"\"\"\n",
    "    :param next_dX:\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.reshape(next_dX, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交叉熵损失函数\n",
    "def cross_entropy_loss(y_predict, y_true):\n",
    "    \"\"\"\n",
    "    :param y_predict: 预测值,shape (N,d)，N为批量样本数\n",
    "    :param y_true: 真实值,shape(N,d)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    y_shift = y_predict - np.max(y_predict, axis=-1, keepdims=True)\n",
    "    y_exp = np.exp(y_shift)\n",
    "    y_probability = y_exp / np.sum(y_exp, axis=-1,keepdims=True)\n",
    "    loss = np.mean(np.sum(-y_true * np.log(y_probability), axis=-1))  # 损失函数\n",
    "    dy = y_probability - y_true\n",
    "    return loss, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机梯度下降\n",
    "def _copy_weights_to_zeros(weights):\n",
    "        result = {}\n",
    "        result.keys()\n",
    "        for key in weights.keys():\n",
    "            result[key] = np.zeros_like(weights[key])\n",
    "        return result\n",
    "class SGD(object):\n",
    "    \"\"\"\n",
    "    小批量梯度下降法\n",
    "    \"\"\"\n",
    "    ##初始化权重学习率动量因子迭代次数\n",
    "    def __init__(self, weights, lr=0.01, momentum=0.9, decay=1e-5):\n",
    "        \"\"\"\n",
    "        :param weights: 权重\n",
    "        :param lr: 初始学习率\n",
    "        :param momentum: 动量因子\n",
    "        :param decay: 学习率衰减\n",
    "        \"\"\"\n",
    "        self.v = _copy_weights_to_zeros(weights)  # 累积动量大小\n",
    "        self.iterations = 0  # 迭代次数\n",
    "        self.lr = self.init_lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "\n",
    "    def iterate(self, weights, gradients):\n",
    "        \"\"\"\n",
    "        迭代一次\n",
    "        :param weights: 当前迭代权重\n",
    "        :param gradients: 当前迭代梯度\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 更新学习率\n",
    "        self.lr = self.init_lr / (1 + self.iterations * self.decay)\n",
    "\n",
    "        # 更新动量和梯度\n",
    "        for key in self.v.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] + self.lr * gradients[key]\n",
    "            weights[key] = weights[key] - self.v[key]\n",
    "\n",
    "        # 更新迭代次数\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:(16, 1, 28, 28),y.shape:(16, 10)\n"
     ]
    }
   ],
   "source": [
    "##加载数据#########################################################################\n",
    "def load_mnist_datasets(path='./Data/mnist.pkl.gz'):\n",
    "    if not os.path.exists(path):\n",
    "        raise Exception('Cannot find %s' % path)\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        train_set, val_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, val_set, test_set\n",
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"\n",
    "    把类别标签转换为onehot编码\n",
    "    源自keras\n",
    "    Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "\n",
    "    # Returns\n",
    "        A binary matrix representation of the input. The classes axis\n",
    "        is placed last.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=np.float32)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "#加载数据，分类训练集，验证集，测试集\n",
    "train_set, val_set, test_set = load_mnist_datasets('./Data/mnist.pkl.gz')\n",
    "train_x,val_x,test_x=np.reshape(train_set[0],(-1,1,28,28)),np.reshape(val_set[0],(-1,1,28,28)),np.reshape(test_set[0],(-1,1,28,28))\n",
    "train_y,val_y,test_y=to_categorical(train_set[1]),to_categorical(val_set[1]),to_categorical(test_set[1])\n",
    "#########################################################################\n",
    "# 随机选择训练样本\n",
    "train_num = train_x.shape[0]\n",
    "def train_select(batch_size):\n",
    "    index=np.random.choice(train_num,batch_size)\n",
    "    return train_x[index],train_y[index]\n",
    "\n",
    "x,y= train_select(16)\n",
    "print(\"x.shape:{},y.shape:{}\".format(x.shape,y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化变量\n",
    "batch_size=4\n",
    "steps = 2000  # 迭代次数\n",
    "# 初始化变量保存迭代步数step和loss值\n",
    "steps_value = []\n",
    "loss_value = []\n",
    "\n",
    "# SGD更新梯度\n",
    "sgd=SGD(weights,lr=0.01,decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " step:0.00\n",
      "\n",
      " loss:2.3026451688573015\n",
      " train_acc:0.25\n",
      " val_acc:0.05\n",
      "\n",
      " step:100.00\n",
      "\n",
      " loss:2.3277694748986884\n",
      " train_acc:0.0\n",
      " val_acc:0.145\n",
      "\n",
      " step:200.00\n",
      "\n",
      " loss:2.3183133327507606\n",
      " train_acc:0.0\n",
      " val_acc:0.15\n",
      "\n",
      " step:300.00\n",
      "\n",
      " loss:1.4793794564254608\n",
      " train_acc:0.5\n",
      " val_acc:0.385\n",
      "\n",
      " step:400.00\n",
      "\n",
      " loss:0.24184962245376646\n",
      " train_acc:1.0\n",
      " val_acc:0.58\n",
      "\n",
      " step:500.00\n",
      "\n",
      " loss:0.2739964233391917\n",
      " train_acc:1.0\n",
      " val_acc:0.78\n",
      "\n",
      " step:600.00\n",
      "\n",
      " loss:0.3205762211135013\n",
      " train_acc:1.0\n",
      " val_acc:0.77\n",
      "\n",
      " step:700.00\n",
      "\n",
      " loss:0.5620494604502715\n",
      " train_acc:1.0\n",
      " val_acc:0.83\n",
      "\n",
      " step:800.00\n",
      "\n",
      " loss:0.07199389715016032\n",
      " train_acc:1.0\n",
      " val_acc:0.88\n",
      "\n",
      " step:900.00\n",
      "\n",
      " loss:0.02193582993928735\n",
      " train_acc:1.0\n",
      " val_acc:0.895\n",
      "\n",
      " step:1000.00\n",
      "\n",
      " loss:0.06693317052460647\n",
      " train_acc:1.0\n",
      " val_acc:0.865\n",
      "\n",
      " step:1100.00\n",
      "\n",
      " loss:0.00016390398457972793\n",
      " train_acc:1.0\n",
      " val_acc:0.91\n",
      "\n",
      " step:1200.00\n",
      "\n",
      " loss:0.06820998779239927\n",
      " train_acc:1.0\n",
      " val_acc:0.785\n",
      "\n",
      " step:1300.00\n",
      "\n",
      " loss:0.08254667342903585\n",
      " train_acc:1.0\n",
      " val_acc:0.84\n",
      "\n",
      " step:1400.00\n",
      "\n",
      " loss:0.0007852588524742863\n",
      " train_acc:1.0\n",
      " val_acc:0.93\n",
      "\n",
      " step:1500.00\n",
      "\n",
      " loss:0.844353001056803\n",
      " train_acc:0.75\n",
      " val_acc:0.84\n",
      "\n",
      " step:1600.00\n",
      "\n",
      " loss:0.15732650954910052\n",
      " train_acc:1.0\n",
      " val_acc:0.895\n",
      "\n",
      " step:1700.00\n",
      "\n",
      " loss:0.01479619497042965\n",
      " train_acc:1.0\n",
      " val_acc:0.95\n",
      "\n",
      " step:1800.00\n",
      "\n",
      " loss:0.9545468033500651\n",
      " train_acc:0.75\n",
      " val_acc:0.875\n",
      "\n",
      " step:1900.00\n",
      "\n",
      " loss:0.013953374287784516\n",
      " train_acc:1.0\n",
      " val_acc:0.885\n",
      "\n",
      " final result test_acc:0.9341;  val_acc:0.9396\n"
     ]
    }
   ],
   "source": [
    "for s in range(steps):\n",
    "    X,y=train_select(batch_size)\n",
    "    \n",
    "    forward(X)  # 前向过程\n",
    "    loss=backward(X,y)  # 反向过程\n",
    "    sgd.iterate(weights,gradients)  # 更新迭代次数\n",
    "\n",
    "    if s % 100 ==0: # 每100次打印一次损失和正确率\n",
    "        print(\"\\n step:{:.2f}\".format(s))\n",
    "        print(\"\\n loss:{}\".format(loss))\n",
    "        idx=np.random.choice(len(val_x),200)\n",
    "        print(\" train_acc:{}\".format(get_accuracy(X,y)))\n",
    "        print(\" val_acc:{}\".format(get_accuracy(val_x[idx],val_y[idx])))\n",
    "        \n",
    "        steps_value.append(s)\n",
    "        loss_value.append(loss)\n",
    "        \n",
    "print(\"\\n final result test_acc:{};  val_acc:{}\".\n",
    "      format(get_accuracy(test_x,test_y),get_accuracy(val_x,val_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
